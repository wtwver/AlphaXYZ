{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "from tqdm.notebook import trange\n",
    "\n",
    "torch.manual_seed(0)\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CarRacing:\n",
    "    def __init__(self, sequence_lenth=3, skip_frames=3, clip_reward=True, time_out_tolerance=100, time_out=25, render=False):\n",
    "        self.env = gym.make('CarRacing-v3', continuous=False, domain_randomize=False, render_mode='human' if render else 'rgb_array')\n",
    "        self.action_size = self.env.action_space.n\n",
    "        self.obs_stack = []\n",
    "        self.sequence_lenth = sequence_lenth\n",
    "        self.skip_frames = skip_frames\n",
    "        self.clip_reward = clip_reward\n",
    "        self.time_out_tolerance = time_out_tolerance\n",
    "        self.time_out = time_out\n",
    "        self.counter = 0\n",
    "        self.negative_reward_counter = 0\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"CarRacing\"\n",
    "\n",
    "    def get_initial_state(self):\n",
    "        self.counter = 0\n",
    "        self.negative_reward_counter = 0\n",
    "        observation, _ = self.env.reset()\n",
    "        action, reward, is_terminal = 0, 0, False \n",
    "        observation = self.get_encoded_observation(observation, action)\n",
    "        self.obs_stack = [observation for _ in range(self.sequence_lenth)]\n",
    "        return np.stack(self.obs_stack), reward, is_terminal\n",
    "\n",
    "    def step(self, action):\n",
    "        reward = 0\n",
    "        for _ in range(self.skip_frames + 1):\n",
    "            observation, r, is_terminal, _, _ = self.env.step(action)\n",
    "            reward += r\n",
    "            if is_terminal:\n",
    "                break\n",
    "        observation = self.get_encoded_observation(observation, action)\n",
    "        self.obs_stack = self.obs_stack[1:] + [observation]\n",
    "        if self.clip_reward:\n",
    "            reward = np.clip(reward, -float('inf'), 1)\n",
    "        if reward < 0 and self.counter > self.time_out_tolerance:\n",
    "            self.negative_reward_counter += 1\n",
    "            if self.negative_reward_counter >= self.time_out:\n",
    "                is_terminal = True\n",
    "                self.env.close()\n",
    "        else:\n",
    "            self.counter += 1\n",
    "        return np.stack(self.obs_stack), reward, is_terminal\n",
    "\n",
    "    def get_encoded_observation(self, observation, action):\n",
    "        obs = Image.fromarray(observation.copy())\n",
    "        obs = obs.crop((0, 0, 96, 84))\n",
    "        obs = obs.convert('L')\n",
    "        obs = np.array(obs)\n",
    "        actionPlane = np.full((12, 96), action * (255 / self.action_size), dtype=np.float32)\n",
    "        obs = np.concatenate((obs, actionPlane), axis=0)\n",
    "        obs /= 255\n",
    "        return obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, args, game):\n",
    "        self.memory = []\n",
    "        self.trajectories = []\n",
    "        self.args = args\n",
    "        self.game = game\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.trajectories)\n",
    "\n",
    "    def empty(self):\n",
    "        self.memory = []\n",
    "        self.trajectories = []\n",
    "\n",
    "    def build_trajectories(self):\n",
    "        for i in range(len(self.memory)):\n",
    "            observation, action, policy, reward, _, game_idx = self.memory[i]\n",
    "            policy_list, action_list, value_list, reward_list = [policy], [action], [], [reward]\n",
    "\n",
    "            # value bootstrap for N-step return\n",
    "            # value starts at root value n steps ahead\n",
    "            if i + self.args['N'] + 1 < len(self.memory) and self.memory[i + self.args['N'] + 1][5] == game_idx:\n",
    "                value = self.memory[i + self.args['N'] + 1][4] * self.args['gamma'] ** self.args['N']\n",
    "            else:\n",
    "                value = 0\n",
    "            # add discounted rewards until end of game or N steps\n",
    "            for n in range(2, self.args['N'] + 2):\n",
    "                if i + n < len(self.memory) and self.memory[i + n][5] == game_idx:\n",
    "                    _, _, _, reward, _, _ = self.memory[i + n]\n",
    "                    value += reward * self.args['gamma'] ** (n - 2)\n",
    "                else:\n",
    "                    break\n",
    "            value_list.append(value)\n",
    "\n",
    "            for k in range(1, self.args['K'] + 1):\n",
    "                if i + k < len(self.memory) and self.memory[i + k][5] == game_idx:\n",
    "                    _, action, policy, reward, _, _ = self.memory[i + k]\n",
    "                    action_list.append(action)\n",
    "                    policy_list.append(policy)\n",
    "                    reward_list.append(reward)\n",
    "\n",
    "                    if i + k + self.args['N'] + 1 < len(self.memory) and self.memory[i + k + self.args['N'] + 1][5] == game_idx:\n",
    "                        value = self.memory[i + k + self.args['N'] + 1][4] * self.args['gamma'] ** self.args['N']\n",
    "                    else:\n",
    "                        value = 0\n",
    "                    for n in range(2, self.args['N'] + 2):\n",
    "                        if i + k + n < len(self.memory) and self.memory[i + k + n][5] == game_idx:\n",
    "                            _, _, _, reward, _, _ = self.memory[i + k + n]\n",
    "                            value += reward * self.args['gamma'] ** (n - 2)\n",
    "                        else:\n",
    "                            break\n",
    "                    value_list.append(value)\n",
    "\n",
    "                else:\n",
    "                    action_list.append(np.random.choice(self.game.action_size))\n",
    "                    policy_list.append(np.full(self.game.action_size, 1 / self.game.action_size))\n",
    "                    value_list.append(0)\n",
    "                    reward_list.append(0)\n",
    "\n",
    "            policy_list = np.stack(policy_list)\n",
    "            self.trajectories.append((observation, action_list, policy_list, value_list, reward_list))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MuZero(nn.Module):\n",
    "    def __init__(self, game, device):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "\n",
    "        self.value_support = DiscreteSupport(-20, 20)\n",
    "        self.reward_support = DiscreteSupport(-5, 5)\n",
    "        \n",
    "        self.predictionFunction = PredictionFunction(game, self.value_support)\n",
    "        self.dynamicsFunction = DynamicsFunction(self.reward_support)\n",
    "        self.representationFunction = RepresentationFunction(game)\n",
    "\n",
    "    def predict(self, hidden_state):\n",
    "        return self.predictionFunction(hidden_state)\n",
    "\n",
    "    def represent(self, observation):\n",
    "        return self.representationFunction(observation)\n",
    "\n",
    "    def dynamics(self, hidden_state, action):\n",
    "        actionArr = torch.zeros((hidden_state.shape[0], 1, 6, 6), device=hidden_state.device, dtype=torch.float32)\n",
    "        for i, a in enumerate(action):\n",
    "            actionArr[i, 0, a] = 1\n",
    "        x = torch.cat((hidden_state, actionArr), dim=1)\n",
    "        return self.dynamicsFunction(x)\n",
    "\n",
    "    def inverse_value_transform(self, value):\n",
    "        return self.inverse_scalar_transform(value, self.value_support)\n",
    "\n",
    "    def inverse_reward_transform(self, reward):\n",
    "        return self.inverse_scalar_transform(reward, self.reward_support)\n",
    "\n",
    "    def inverse_scalar_transform(self, output, support):\n",
    "        output_propbs = torch.softmax(output, dim=1)\n",
    "        output_support = torch.ones(output_propbs.shape, dtype=torch.float32, device=self.device)\n",
    "        output_support[:, :] = torch.tensor([x for x in support.range], device=self.device)\n",
    "        scalar_output = (output_propbs * output_support).sum(dim=1, keepdim=True)\n",
    "\n",
    "        epsilon = 0.001\n",
    "        sign = torch.sign(scalar_output)\n",
    "        inverse_scalar_output = sign * (((torch.sqrt(1 + 4 * epsilon * (torch.abs(scalar_output) + 1 + epsilon)) - 1) / (2 * epsilon)) ** 2 - 1)\n",
    "        return inverse_scalar_output\n",
    "\n",
    "    def scalar_transform(self, x):\n",
    "        epsilon = 0.001\n",
    "        sign = torch.sign(x)\n",
    "        output = sign * (torch.sqrt(torch.abs(x) + 1) - 1 + epsilon * x)\n",
    "        return output\n",
    "\n",
    "    def value_phi(self, x):\n",
    "        return self._phi(x, self.value_support.min, self.value_support.max, self.value_support.size)\n",
    "\n",
    "    def reward_phi(self, x):\n",
    "        return self._phi(x, self.reward_support.min, self.reward_support.max, self.reward_support.size)\n",
    "\n",
    "    def _phi(self, x, min, max, set_size):\n",
    "        x.clamp_(min, max)\n",
    "        x_low = x.floor()\n",
    "        x_high = x.ceil()\n",
    "        p_high = (x - x_low)\n",
    "        p_low = 1 - p_high\n",
    "\n",
    "        target = torch.zeros(x.shape[0], x.shape[1], set_size).to(x.device)\n",
    "        x_high_idx, x_low_idx = x_high - min, x_low - min\n",
    "        target.scatter_(2, x_high_idx.long().unsqueeze(-1), p_high.unsqueeze(-1))\n",
    "        target.scatter_(2, x_low_idx.long().unsqueeze(-1), p_low.unsqueeze(-1))\n",
    "        return target\n",
    "\n",
    "# Creates hidden state + reward based on old hidden state and action \n",
    "class DynamicsFunction(nn.Module):\n",
    "    def __init__(self, reward_support, num_resBlocks=4, hidden_planes=32):\n",
    "        super().__init__()\n",
    "        self.startBlock = nn.Sequential(\n",
    "            nn.Conv2d(4, hidden_planes, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(hidden_planes),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.resBlocks = nn.ModuleList([ResBlock(hidden_planes, hidden_planes) for _ in range(num_resBlocks)])\n",
    "        self.endBlock = nn.Sequential(\n",
    "            nn.Conv2d(hidden_planes, 3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(3),\n",
    "        )\n",
    "\n",
    "        self.rewardBlock = nn.Sequential(\n",
    "            nn.Conv2d(3, 1, kernel_size=1, stride=1, padding=0),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(6 * 6, reward_support.size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.startBlock(x)\n",
    "        for block in self.resBlocks:\n",
    "            x = block(x)\n",
    "        x = self.endBlock(x)\n",
    "        reward = self.rewardBlock(x)\n",
    "        return x, reward\n",
    "    \n",
    "# Creates policy and value based on hidden state\n",
    "class PredictionFunction(nn.Module):\n",
    "    def __init__(self, game, value_support, num_resBlocks=4, hidden_planes=32):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.startBlock = nn.Sequential(\n",
    "            nn.Conv2d(3, hidden_planes, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(hidden_planes),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.resBlocks = nn.ModuleList([ResBlock(hidden_planes, hidden_planes) for _ in range(num_resBlocks)])\n",
    "\n",
    "        self.policy_head = nn.Sequential(\n",
    "            nn.Conv2d(hidden_planes, 16, kernel_size=1, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(16 * 6 * 6, game.action_size)\n",
    "        )\n",
    "        self.value_head = nn.Sequential(\n",
    "            nn.Conv2d(hidden_planes, 3, kernel_size=1, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(3),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(3 * 6 * 6, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, value_support.size),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.startBlock(x)\n",
    "        for block in self.resBlocks:\n",
    "            x = block(x)\n",
    "        p = self.policy_head(x)\n",
    "        v = self.value_head(x)\n",
    "        return p, v\n",
    "\n",
    "# Creates initial hidden state based on observation | several observations\n",
    "class RepresentationFunction(nn.Module):\n",
    "    def __init__(self, game, hidden_planes=32):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv2d(game.sequence_lenth, hidden_planes // 2, kernel_size=3, stride=2, padding=1), # 48x48\n",
    "            nn.BatchNorm2d(hidden_planes // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(hidden_planes // 2, hidden_planes, kernel_size=3, stride=2, padding=1), # 24x24\n",
    "            nn.BatchNorm2d(hidden_planes),\n",
    "            nn.ReLU(),\n",
    "            # ResBlock(hidden_planes, hidden_planes),\n",
    "            nn.Conv2d(hidden_planes, hidden_planes, kernel_size=3, stride=2, padding=1), # 12x12\n",
    "            nn.BatchNorm2d(hidden_planes),\n",
    "            nn.ReLU(),\n",
    "            # ResBlock(hidden_planes, hidden_planes),\n",
    "            nn.Conv2d(hidden_planes, hidden_planes // 2, kernel_size=3, stride=2, padding=1), # 6x6\n",
    "            nn.BatchNorm2d(hidden_planes // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(hidden_planes // 2, 3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(3)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        return x\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, padding=1, stride=stride, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, padding=1, stride=stride, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += residual\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "class DiscreteSupport:\n",
    "    def __init__(self, min, max):\n",
    "        assert min < max\n",
    "        self.min = min\n",
    "        self.max = max\n",
    "        self.range = range(min, max + 1)\n",
    "        self.size = len(self.range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MinMaxStats:\n",
    "    def __init__(self, known_bounds):\n",
    "        self.maximum = known_bounds['max'] if known_bounds else -float('inf')\n",
    "        self.minimum = known_bounds['min'] if known_bounds else float('inf')\n",
    "\n",
    "    def update(self, value):\n",
    "        self.maximum = max(self.maximum, value)\n",
    "        self.minimum = min(self.minimum, value)\n",
    "\n",
    "    def normalize(self, value):\n",
    "        if self.maximum > self.minimum:\n",
    "            return (value - self.minimum) / (self.maximum - self.minimum)\n",
    "        return value\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, state, reward, prior, muZero, args, game, parent=None, action_taken=None, visit_count=0):\n",
    "        self.state = state\n",
    "        self.reward = reward\n",
    "        self.children = []\n",
    "        self.parent = parent\n",
    "        self.total_value = 0\n",
    "        self.visit_count = visit_count # Should start at 1 for root node\n",
    "        self.prior = prior\n",
    "        self.muZero = muZero\n",
    "        self.action_taken = action_taken\n",
    "        self.args = args\n",
    "        self.game = game\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def expand(self, action_probs):\n",
    "        actions = [a for a in range(self.game.action_size) if action_probs[a] > 0]\n",
    "        expand_state = self.state.copy()\n",
    "        expand_state = np.expand_dims(expand_state, axis=0).repeat(len(actions), axis=0)\n",
    "\n",
    "        expand_state, reward = self.muZero.dynamics(\n",
    "            torch.tensor(expand_state, dtype=torch.float32, device=self.muZero.device), actions)\n",
    "        expand_state = expand_state.cpu().numpy()\n",
    "        reward = self.muZero.inverse_reward_transform(reward).cpu().numpy().flatten()\n",
    "        \n",
    "        for i, a in enumerate(actions):\n",
    "            child = Node(\n",
    "                expand_state[i],\n",
    "                reward[i],\n",
    "                action_probs[a],\n",
    "                self.muZero,\n",
    "                self.args,\n",
    "                self.game,\n",
    "                parent=self,\n",
    "                action_taken=a,\n",
    "            )\n",
    "            self.children.append(child)\n",
    "\n",
    "    def backpropagate(self, value, minMaxStats):\n",
    "        self.total_value += value\n",
    "        self.visit_count += 1\n",
    "        minMaxStats.update(self.value())\n",
    "        if self.parent is not None:\n",
    "            value = self.reward + self.args['gamma'] * value\n",
    "            self.parent.backpropagate(value, minMaxStats)\n",
    "\n",
    "    def is_expanded(self):\n",
    "        return len(self.children) > 0\n",
    "\n",
    "    def value(self):\n",
    "        if self.visit_count == 0:\n",
    "            return 0\n",
    "        return self.total_value / self.visit_count\n",
    "\n",
    "    def select_child(self, minMaxStats):\n",
    "        best_score = -np.inf\n",
    "        best_child = None\n",
    "\n",
    "        for child in self.children:\n",
    "            ucb_score = self.get_ucb_score(child, minMaxStats)\n",
    "            if ucb_score > best_score:\n",
    "                best_score = ucb_score\n",
    "                best_child = child\n",
    "\n",
    "        return best_child\n",
    "\n",
    "    def get_ucb_score(self, child, minMaxStats):\n",
    "        pb_c = math.log((self.visit_count + self.args[\"pb_c_base\"] + 1) /\n",
    "                  self.args[\"pb_c_base\"]) + self.args[\"pb_c_init\"]\n",
    "        pb_c *= math.sqrt(self.visit_count) / (child.visit_count + 1)\n",
    "        prior_score = pb_c * child.prior\n",
    "        if child.visit_count > 0:\n",
    "            value_score = minMaxStats.normalize(child.reward + self.args['gamma'] * child.value())\n",
    "        else:\n",
    "            # value_score = 0\n",
    "            value_score = minMaxStats.normalize(child.reward)\n",
    "        return prior_score + value_score\n",
    "\n",
    "class MCTS:\n",
    "    def __init__(self, muZero, game, args):\n",
    "        self.muZero = muZero\n",
    "        self.game = game\n",
    "        self.args = args\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def search(self, state, reward):\n",
    "        minMaxStats = MinMaxStats(self.args['known_bounds'])\n",
    "        hidden_state = self.muZero.represent(\n",
    "            torch.tensor(state, dtype=torch.float32, device=self.muZero.device).unsqueeze(0)\n",
    "        )\n",
    "        action_probs, _ = self.muZero.predict(hidden_state)\n",
    "        hidden_state = hidden_state.cpu().numpy().squeeze(0)\n",
    "        \n",
    "        root = Node(hidden_state, reward, 0, self.muZero, self.args, self.game, visit_count=1)\n",
    "\n",
    "        action_probs = torch.softmax(action_probs, dim=1).cpu().numpy().squeeze(0)\n",
    "        action_probs = action_probs * (1 - self.args['dirichlet_epsilon']) + self.args['dirichlet_epsilon'] * np.random.dirichlet([self.args['dirichlet_alpha']] * self.game.action_size)\n",
    "        action_probs /= np.sum(action_probs)\n",
    "\n",
    "        root.expand(action_probs)\n",
    "\n",
    "        for simulation in range(self.args['num_mcts_runs']):\n",
    "            node = root\n",
    "\n",
    "            while node.is_expanded():\n",
    "                node = node.select_child(minMaxStats)\n",
    "\n",
    "            action_probs, value = self.muZero.predict(\n",
    "                torch.tensor(node.state, dtype=torch.float32, device=self.muZero.device).unsqueeze(0)\n",
    "            )\n",
    "            action_probs = torch.softmax(action_probs, dim=1).cpu().numpy().squeeze(0)\n",
    "            value = self.muZero.inverse_value_transform(value).item()\n",
    "\n",
    "            node.expand(action_probs)\n",
    "            node.backpropagate(value, minMaxStats)\n",
    "\n",
    "        return root\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, muZero, optimizer, game, args):\n",
    "        self.muZero = muZero\n",
    "        self.optimizer = optimizer\n",
    "        self.game = game\n",
    "        self.args = args\n",
    "        self.mcts = MCTS(self.muZero, self.game, self.args)\n",
    "        self.replayBuffer = ReplayBuffer(self.args, self.game)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def self_play(self, game_idx):\n",
    "        game_memory = []\n",
    "        observation, reward, is_terminal = self.game.get_initial_state()\n",
    "\n",
    "        while True:\n",
    "            root = self.mcts.search(observation.copy(), reward)\n",
    "\n",
    "            action_probs = [0] * self.game.action_size\n",
    "            for child in root.children:\n",
    "                action_probs[child.action_taken] = child.visit_count\n",
    "            action_probs /= np.sum(action_probs)\n",
    "\n",
    "            # sample action from the mcts policy | based on temperature\n",
    "            if self.args['temperature'] == 0:\n",
    "                action = np.argmax(action_probs)\n",
    "            elif self.args['temperature'] == float('inf'):\n",
    "                action = np.random.choice([r for r in range(self.game.action_size) if action_probs[r] > 0])\n",
    "            else:\n",
    "                temperature_action_probs = action_probs ** (1 / self.args['temperature'])\n",
    "                temperature_action_probs /= np.sum(temperature_action_probs)\n",
    "                action = np.random.choice(len(temperature_action_probs), p=temperature_action_probs)\n",
    "\n",
    "            game_memory.append((observation, action, action_probs, root.total_value / root.visit_count, reward))\n",
    "            observation, reward, is_terminal = self.game.step(action)\n",
    "\n",
    "            if is_terminal:\n",
    "                return_memory = []\n",
    "                for hist_state, hist_action, hist_action_probs, hist_root_value, hist_reward in game_memory:\n",
    "                    return_memory.append((\n",
    "                        hist_state,\n",
    "                        hist_action, \n",
    "                        hist_action_probs,\n",
    "                        hist_reward,\n",
    "                        hist_root_value,\n",
    "                        game_idx,\n",
    "                    ))\n",
    "                return return_memory\n",
    "\n",
    "    def train(self):\n",
    "        random.shuffle(self.replayBuffer.trajectories)\n",
    "        for batchIdx in range(0, len(self.replayBuffer), self.args['batch_size']): \n",
    "            state, action, policy, value, reward = list(zip(*self.replayBuffer.trajectories[batchIdx:batchIdx+self.args['batch_size']]))\n",
    "\n",
    "            state = torch.tensor(np.stack(state), dtype=torch.float32, device=self.device)\n",
    "            action = np.array(action)\n",
    "            policy = torch.tensor(np.stack(policy), dtype=torch.float32, device=self.device)\n",
    "            value = torch.tensor(np.array(value), dtype=torch.float32, device=self.device)\n",
    "            reward = torch.tensor(np.array(reward), dtype=torch.float32, device=self.device)\n",
    "\n",
    "            transformed_reward = self.muZero.scalar_transform(reward)\n",
    "            phi_reward = self.muZero.reward_phi(transformed_reward)\n",
    "            transformed_value = self.muZero.scalar_transform(value)\n",
    "            phi_value = self.muZero.value_phi(transformed_value)\n",
    "\n",
    "            state = self.muZero.represent(state)\n",
    "            out_policy, out_value = self.muZero.predict(state)\n",
    "\n",
    "            policy_loss = F.cross_entropy(out_policy, policy[:, 0]) \n",
    "            value_loss = self.scalar_value_loss(out_value, phi_value[:, 0])\n",
    "            reward_loss = torch.zeros(value_loss.shape, device=self.device)\n",
    "\n",
    "            for k in range(1, self.args['K'] + 1):\n",
    "                state, out_reward = self.muZero.dynamics(state, action[:, k - 1])\n",
    "                reward_loss += self.scalar_reward_loss(out_reward, phi_reward[:, k])\n",
    "                state.register_hook(lambda grad: grad * 0.5)\n",
    "\n",
    "                out_policy, out_value = self.muZero.predict(state)\n",
    "\n",
    "                policy_loss += F.cross_entropy(out_policy, policy[:, k])\n",
    "                value_loss += self.scalar_value_loss(out_value, phi_value[:, k])\n",
    "\n",
    "            loss = (value_loss * self.args['value_loss_weight'] + policy_loss + reward_loss).mean()\n",
    "            loss.register_hook(lambda grad: grad * 1 / self.args['K'])\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.muZero.parameters(), self.args['max_grad_norm'])\n",
    "            self.optimizer.step()\n",
    "\n",
    "    def run(self):\n",
    "        for iteration in range(self.args['num_iterations']):\n",
    "            print(f\"iteration: {iteration}\")\n",
    "            self.replayBuffer.empty()\n",
    "\n",
    "            self.muZero.eval()\n",
    "            for train_game_idx in (self_play_bar := trange(self.args['num_train_games'], desc=\"train_game\")):\n",
    "                self.replayBuffer.memory += self.self_play(train_game_idx + iteration * self.args['num_train_games'])\n",
    "                self_play_bar.set_description(f\"Avg. steps per Game: {len(self.replayBuffer.memory) / (train_game_idx + 1):.2f}\")\n",
    "            self.replayBuffer.build_trajectories()\n",
    "\n",
    "            self.muZero.train()\n",
    "            for epoch in trange(self.args['num_epochs'], desc=\"epochs\"):\n",
    "                self.train()\n",
    "\n",
    "            torch.save(self.muZero.state_dict(), f\"../../Environments/{self.game}/Models/model_{iteration}.pt\")\n",
    "            torch.save(self.optimizer.state_dict(), f\"../../Environments/{self.game}/Models/optimizer_{iteration}.pt\")\n",
    "\n",
    "    def scalar_reward_loss(self, prediction, target):\n",
    "        return -(torch.log_softmax(prediction, dim=1) * target).sum(1)\n",
    "\n",
    "    def scalar_value_loss(self, prediction, target):\n",
    "        return -(torch.log_softmax(prediction, dim=1) * target).sum(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41cb6f7f381e488391cbe01d788c41a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train_game:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'astype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 32\u001b[0m\n\u001b[1;32m     29\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./Environments/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgame\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/Models/optimizer.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m, map_location\u001b[38;5;241m=\u001b[39mdevice))\n\u001b[1;32m     31\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(muZero, optimizer, game, args)\n\u001b[0;32m---> 32\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[40], line 97\u001b[0m, in \u001b[0;36mTrainer.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmuZero\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m train_game_idx \u001b[38;5;129;01min\u001b[39;00m (self_play_bar \u001b[38;5;241m:=\u001b[39m trange(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_train_games\u001b[39m\u001b[38;5;124m'\u001b[39m], desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_game\u001b[39m\u001b[38;5;124m\"\u001b[39m)):\n\u001b[0;32m---> 97\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplayBuffer\u001b[38;5;241m.\u001b[39mmemory \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_play\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_game_idx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43miteration\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnum_train_games\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m     self_play_bar\u001b[38;5;241m.\u001b[39mset_description(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAvg. steps per Game: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplayBuffer\u001b[38;5;241m.\u001b[39mmemory)\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39m(train_game_idx\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplayBuffer\u001b[38;5;241m.\u001b[39mbuild_trajectories()\n",
      "Cell \u001b[0;32mIn[40], line 34\u001b[0m, in \u001b[0;36mTrainer.self_play\u001b[0;34m(self, game_idx)\u001b[0m\n\u001b[1;32m     31\u001b[0m     action \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(\u001b[38;5;28mlen\u001b[39m(temperature_action_probs), p\u001b[38;5;241m=\u001b[39mtemperature_action_probs)\n\u001b[1;32m     33\u001b[0m game_memory\u001b[38;5;241m.\u001b[39mappend((observation, action, action_probs, root\u001b[38;5;241m.\u001b[39mtotal_value \u001b[38;5;241m/\u001b[39m root\u001b[38;5;241m.\u001b[39mvisit_count, reward))\n\u001b[0;32m---> 34\u001b[0m observation, reward, is_terminal \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgame\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_terminal:\n\u001b[1;32m     37\u001b[0m     return_memory \u001b[38;5;241m=\u001b[39m []\n",
      "Cell \u001b[0;32mIn[36], line 29\u001b[0m, in \u001b[0;36mCarRacing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     27\u001b[0m reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskip_frames \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m---> 29\u001b[0m     observation, r, is_terminal, _, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m     reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m r\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_terminal:\n",
      "File \u001b[0;32m~/AlphaXYZ/AlphaGo/.venv/lib/python3.12/site-packages/gymnasium/wrappers/common.py:125\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep\u001b[39m(\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: ActType\n\u001b[1;32m    114\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[ObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[1;32m    115\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    123\u001b[0m \n\u001b[1;32m    124\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 125\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[0;32m~/AlphaXYZ/AlphaGo/.venv/lib/python3.12/site-packages/gymnasium/wrappers/common.py:393\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 393\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/AlphaXYZ/AlphaGo/.venv/lib/python3.12/site-packages/gymnasium/core.py:322\u001b[0m, in \u001b[0;36mWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep\u001b[39m(\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: WrapperActType\n\u001b[1;32m    320\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[1;32m    321\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Uses the :meth:`step` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 322\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/AlphaXYZ/AlphaGo/.venv/lib/python3.12/site-packages/gymnasium/wrappers/common.py:283\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchecked_step \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchecked_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 283\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43menv_step_passive_checker\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "File \u001b[0;32m~/AlphaXYZ/AlphaGo/.venv/lib/python3.12/site-packages/gymnasium/utils/passive_env_checker.py:207\u001b[0m, in \u001b[0;36menv_step_passive_checker\u001b[0;34m(env, action)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"A passive check for the environment step, investigating the returning data then returning the data unchanged.\"\"\"\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;66;03m# We don't check the action as for some environments then out-of-bounds values can be given\u001b[39;00m\n\u001b[0;32m--> 207\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m    209\u001b[0m     result, \u001b[38;5;28mtuple\u001b[39m\n\u001b[1;32m    210\u001b[0m ), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpects step result to be a tuple, actual type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(result)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(result) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m4\u001b[39m:\n",
      "File \u001b[0;32m~/AlphaXYZ/AlphaGo/.venv/lib/python3.12/site-packages/gymnasium/envs/box2d/car_racing.py:544\u001b[0m, in \u001b[0;36mCarRacing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    542\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcar \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    543\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 544\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[43maction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m(np\u001b[38;5;241m.\u001b[39mfloat64)\n\u001b[1;32m    545\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontinuous:\n\u001b[1;32m    546\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcar\u001b[38;5;241m.\u001b[39msteer(\u001b[38;5;241m-\u001b[39maction[\u001b[38;5;241m0\u001b[39m])\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'astype'"
     ]
    }
   ],
   "source": [
    "args = {\n",
    "    'num_iterations': 20,\n",
    "    'num_train_games': 100,\n",
    "    'group_size': 100,\n",
    "    'num_mcts_runs': 50,\n",
    "    'num_epochs': 4,\n",
    "    'batch_size': 64,\n",
    "    'temperature': 1,\n",
    "    'K': 5,\n",
    "    'pb_c_base': 19625,\n",
    "    'pb_c_init': 2,\n",
    "    'N': 10,\n",
    "    'dirichlet_alpha': 0.3,\n",
    "    'dirichlet_epsilon': 0.25,\n",
    "    'gamma': 0.997,\n",
    "    'value_loss_weight': 0.25,\n",
    "    'max_grad_norm': 5,\n",
    "    'known_bounds': {} #{'min': 0, 'max': 1},\n",
    "}\n",
    "\n",
    "LOAD = False\n",
    "\n",
    "game = CarRacing()\n",
    "muZero = MuZero(game, device).to(device)\n",
    "optimizer = torch.optim.Adam(muZero.parameters(), lr=0.001)\n",
    "\n",
    "if LOAD:\n",
    "    muZero.load_state_dict(torch.load(f\"./Environments/{game}/Models/model.pt\", map_location=device))\n",
    "    optimizer.load_state_dict(torch.load(f\"./Environments/{game}/Models/optimizer.pt\", map_location=device))\n",
    "\n",
    "trainer = Trainer(muZero, optimizer, game, args)\n",
    "trainer.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
