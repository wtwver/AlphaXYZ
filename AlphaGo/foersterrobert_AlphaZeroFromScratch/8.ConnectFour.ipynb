{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d090f5e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.2\n",
      "2.6.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(np.__version__)\n",
    "\n",
    "\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "from tqdm.notebook import trange\n",
    "\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a097e1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TicTacToe:\n",
    "    def __init__(self):\n",
    "        self.row_count = 3\n",
    "        self.column_count = 3\n",
    "        self.action_size = self.row_count * self.column_count\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return \"TicTacToe\"\n",
    "        \n",
    "    def get_initial_state(self):\n",
    "        return np.zeros((self.row_count, self.column_count))\n",
    "    \n",
    "    def get_next_state(self, state, action, player):\n",
    "        row = action // self.column_count\n",
    "        column = action % self.column_count\n",
    "        state[row, column] = player\n",
    "        return state\n",
    "    \n",
    "    def get_valid_moves(self, state):\n",
    "        return (state.reshape(-1) == 0).astype(np.uint8)\n",
    "    \n",
    "    def check_win(self, state, action):\n",
    "        if action == None:\n",
    "            return False\n",
    "        \n",
    "        row = action // self.column_count\n",
    "        column = action % self.column_count\n",
    "        player = state[row, column]\n",
    "        \n",
    "        return (\n",
    "            np.sum(state[row, :]) == player * self.column_count\n",
    "            or np.sum(state[:, column]) == player * self.row_count\n",
    "            or np.sum(np.diag(state)) == player * self.row_count\n",
    "            or np.sum(np.diag(np.flip(state, axis=0))) == player * self.row_count\n",
    "        )\n",
    "    \n",
    "    def get_value_and_terminated(self, state, action):\n",
    "        if self.check_win(state, action):\n",
    "            return 1, True\n",
    "        if np.sum(self.get_valid_moves(state)) == 0:\n",
    "            return 0, True\n",
    "        return 0, False\n",
    "    \n",
    "    def get_opponent(self, player):\n",
    "        return -player\n",
    "    \n",
    "    def get_opponent_value(self, value):\n",
    "        return -value\n",
    "    \n",
    "    def change_perspective(self, state, player):\n",
    "        return state * player\n",
    "    \n",
    "    def get_encoded_state(self, state):\n",
    "        encoded_state = np.stack(\n",
    "            (state == -1, state == 0, state == 1)\n",
    "        ).astype(np.float32)\n",
    "        \n",
    "        return encoded_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "682c4ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConnectFour:\n",
    "    def __init__(self):\n",
    "        self.row_count = 6\n",
    "        self.column_count = 7\n",
    "        self.action_size = self.column_count\n",
    "        self.in_a_row = 4\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return \"ConnectFour\"\n",
    "        \n",
    "    def get_initial_state(self):\n",
    "        return np.zeros((self.row_count, self.column_count))\n",
    "    \n",
    "    def get_next_state(self, state, action, player):\n",
    "        row = np.max(np.where(state[:, action] == 0))\n",
    "        state[row, action] = player\n",
    "        return state\n",
    "    \n",
    "    def get_valid_moves(self, state):\n",
    "        return (state[0] == 0).astype(np.uint8)\n",
    "    \n",
    "    def check_win(self, state, action):\n",
    "        if action == None:\n",
    "            return False\n",
    "        \n",
    "        row = np.min(np.where(state[:, action] != 0))\n",
    "        column = action\n",
    "        player = state[row][column]\n",
    "\n",
    "        def count(offset_row, offset_column):\n",
    "            for i in range(1, self.in_a_row):\n",
    "                r = row + offset_row * i\n",
    "                c = action + offset_column * i\n",
    "                if (\n",
    "                    r < 0 \n",
    "                    or r >= self.row_count\n",
    "                    or c < 0 \n",
    "                    or c >= self.column_count\n",
    "                    or state[r][c] != player\n",
    "                ):\n",
    "                    return i - 1\n",
    "            return self.in_a_row - 1\n",
    "\n",
    "        return (\n",
    "            count(1, 0) >= self.in_a_row - 1 # vertical\n",
    "            or (count(0, 1) + count(0, -1)) >= self.in_a_row - 1 # horizontal\n",
    "            or (count(1, 1) + count(-1, -1)) >= self.in_a_row - 1 # top left diagonal\n",
    "            or (count(1, -1) + count(-1, 1)) >= self.in_a_row - 1 # top right diagonal\n",
    "        )\n",
    "    \n",
    "    def get_value_and_terminated(self, state, action):\n",
    "        if self.check_win(state, action):\n",
    "            return 1, True\n",
    "        if np.sum(self.get_valid_moves(state)) == 0:\n",
    "            return 0, True\n",
    "        return 0, False\n",
    "    \n",
    "    def get_opponent(self, player):\n",
    "        return -player\n",
    "    \n",
    "    def get_opponent_value(self, value):\n",
    "        return -value\n",
    "    \n",
    "    def change_perspective(self, state, player):\n",
    "        return state * player\n",
    "    \n",
    "    def get_encoded_state(self, state):\n",
    "        encoded_state = np.stack(\n",
    "            (state == -1, state == 0, state == 1)\n",
    "        ).astype(np.float32)\n",
    "        \n",
    "        return encoded_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02e5b58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, game, num_resBlocks, num_hidden, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.device = device\n",
    "        self.startBlock = nn.Sequential(\n",
    "            nn.Conv2d(3, num_hidden, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(num_hidden),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.backBone = nn.ModuleList(\n",
    "            [ResBlock(num_hidden) for i in range(num_resBlocks)]\n",
    "        )\n",
    "        \n",
    "        self.policyHead = nn.Sequential(\n",
    "            nn.Conv2d(num_hidden, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(32 * game.row_count * game.column_count, game.action_size)\n",
    "        )\n",
    "        \n",
    "        self.valueHead = nn.Sequential(\n",
    "            nn.Conv2d(num_hidden, 3, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(3),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(3 * game.row_count * game.column_count, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "        self.to(device)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.startBlock(x)\n",
    "        for resBlock in self.backBone:\n",
    "            x = resBlock(x)\n",
    "        policy = self.policyHead(x)\n",
    "        value = self.valueHead(x)\n",
    "        return policy, value\n",
    "        \n",
    "        \n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, num_hidden):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(num_hidden, num_hidden, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(num_hidden)\n",
    "        self.conv2 = nn.Conv2d(num_hidden, num_hidden, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(num_hidden)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.bn2(self.conv2(x))\n",
    "        x += residual\n",
    "        x = F.relu(x)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "797c3205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09260089695453644\n",
      "[[ 0.  0. -1.]\n",
      " [ 0. -1.  0.]\n",
      " [ 1.  0.  1.]]\n",
      "tensor([[[[0., 0., 1.],\n",
      "          [0., 1., 0.],\n",
      "          [0., 0., 0.]],\n",
      "\n",
      "         [[1., 1., 0.],\n",
      "          [1., 0., 1.],\n",
      "          [0., 1., 0.]],\n",
      "\n",
      "         [[0., 0., 0.],\n",
      "          [0., 0., 0.],\n",
      "          [1., 0., 1.]]]])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tictactoe = TicTacToe()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "state = tictactoe.get_initial_state()\n",
    "state = tictactoe.get_next_state(state, 2, -1)\n",
    "state = tictactoe.get_next_state(state, 4, -1)\n",
    "state = tictactoe.get_next_state(state, 6, 1)\n",
    "state = tictactoe.get_next_state(state, 8, 1)\n",
    "\n",
    "\n",
    "encoded_state = tictactoe.get_encoded_state(state)\n",
    "\n",
    "tensor_state = torch.tensor(encoded_state, device=device).unsqueeze(0)\n",
    "\n",
    "model = ResNet(tictactoe, 4, 64, device=device)\n",
    "# model.load_state_dict(torch.load('model_2.pt', map_location=device))\n",
    "model.eval()\n",
    "\n",
    "policy, value = model(tensor_state)\n",
    "value = value.item()\n",
    "policy = torch.softmax(policy, axis=1).squeeze(0).detach().cpu().numpy()\n",
    "\n",
    "print(value)\n",
    "\n",
    "print(state)\n",
    "print(tensor_state)\n",
    "\n",
    "plt.bar(range(tictactoe.action_size), policy)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21866526",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, game, args, state, parent=None, action_taken=None, prior=0, visit_count=0):\n",
    "        self.game = game\n",
    "        self.args = args\n",
    "        self.state = state\n",
    "        self.parent = parent\n",
    "        self.action_taken = action_taken\n",
    "        self.prior = prior\n",
    "        \n",
    "        self.children = []\n",
    "        \n",
    "        self.visit_count = visit_count\n",
    "        self.value_sum = 0\n",
    "        \n",
    "    def is_fully_expanded(self):\n",
    "        return len(self.children) > 0\n",
    "    \n",
    "    def select(self):\n",
    "        best_child = None\n",
    "        best_ucb = -np.inf\n",
    "        \n",
    "        for child in self.children:\n",
    "            ucb = self.get_ucb(child)\n",
    "            if ucb > best_ucb:\n",
    "                best_child = child\n",
    "                best_ucb = ucb\n",
    "                \n",
    "        return best_child\n",
    "    \n",
    "    def get_ucb(self, child):\n",
    "        if child.visit_count == 0:\n",
    "            q_value = 0\n",
    "        else:\n",
    "            q_value = 1 - ((child.value_sum / child.visit_count) + 1) / 2\n",
    "        return q_value + self.args['C'] * (math.sqrt(self.visit_count) / (child.visit_count + 1)) * child.prior\n",
    "    \n",
    "    def expand(self, policy):\n",
    "        for action, prob in enumerate(policy):\n",
    "            if prob > 0:\n",
    "                child_state = self.state.copy()\n",
    "                child_state = self.game.get_next_state(child_state, action, 1)\n",
    "                child_state = self.game.change_perspective(child_state, player=-1)\n",
    "\n",
    "                child = Node(self.game, self.args, child_state, self, action, prob)\n",
    "                self.children.append(child)\n",
    "                \n",
    "        return child\n",
    "            \n",
    "    def backpropagate(self, value):\n",
    "        self.value_sum += value\n",
    "        self.visit_count += 1\n",
    "        \n",
    "        value = self.game.get_opponent_value(value)\n",
    "        if self.parent is not None:\n",
    "            self.parent.backpropagate(value)  \n",
    "\n",
    "\n",
    "class MCTS:\n",
    "    def __init__(self, game, args, model):\n",
    "        self.game = game\n",
    "        self.args = args\n",
    "        self.model = model\n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def search(self, state):\n",
    "        root = Node(self.game, self.args, state, visit_count=1)\n",
    "        \n",
    "        policy, _ = self.model(\n",
    "            torch.tensor(self.game.get_encoded_state(state), device=self.model.device).unsqueeze(0)\n",
    "        )\n",
    "        policy = torch.softmax(policy, axis=1).squeeze(0).cpu().numpy()\n",
    "        policy = (1 - self.args['dirichlet_epsilon']) * policy + self.args['dirichlet_epsilon'] \\\n",
    "            * np.random.dirichlet([self.args['dirichlet_alpha']] * self.game.action_size)\n",
    "        \n",
    "        valid_moves = self.game.get_valid_moves(state)\n",
    "        policy *= valid_moves\n",
    "        policy /= np.sum(policy)\n",
    "        root.expand(policy)\n",
    "        \n",
    "        for search in range(self.args['num_searches']):\n",
    "            node = root\n",
    "            \n",
    "            while node.is_fully_expanded():\n",
    "                node = node.select()\n",
    "                \n",
    "            value, is_terminal = self.game.get_value_and_terminated(node.state, node.action_taken)\n",
    "            value = self.game.get_opponent_value(value)\n",
    "            \n",
    "            if not is_terminal:\n",
    "                policy, value = self.model(\n",
    "                    torch.tensor(self.game.get_encoded_state(node.state), device=self.model.device).unsqueeze(0)\n",
    "                )\n",
    "                policy = torch.softmax(policy, axis=1).squeeze(0).cpu().numpy()\n",
    "                valid_moves = self.game.get_valid_moves(node.state)\n",
    "                policy *= valid_moves\n",
    "                policy /= np.sum(policy)\n",
    "                \n",
    "                value = value.item()\n",
    "                \n",
    "                node.expand(policy)\n",
    "                \n",
    "            node.backpropagate(value)    \n",
    "            \n",
    "            \n",
    "        action_probs = np.zeros(self.game.action_size)\n",
    "        for child in root.children:\n",
    "            action_probs[child.action_taken] = child.visit_count\n",
    "        action_probs /= np.sum(action_probs)\n",
    "        return action_probs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3b28ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphaZero:\n",
    "    def __init__(self, model, optimizer, game, args):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.game = game\n",
    "        self.args = args\n",
    "        self.mcts = MCTS(game, args, model)\n",
    "        \n",
    "    def selfPlay(self):\n",
    "        memory = []\n",
    "        player = 1\n",
    "        state = self.game.get_initial_state()\n",
    "        \n",
    "        while True:\n",
    "            neutral_state = self.game.change_perspective(state, player)\n",
    "            action_probs = self.mcts.search(neutral_state)\n",
    "            \n",
    "            memory.append((neutral_state, action_probs, player))\n",
    "            \n",
    "            temperature_action_probs = action_probs ** (1 / self.args['temperature'])\n",
    "            action = np.random.choice(self.game.action_size, p=action_probs) # change to temperature_action_probs\n",
    "            \n",
    "            state = self.game.get_next_state(state, action, player)\n",
    "            \n",
    "            value, is_terminal = self.game.get_value_and_terminated(state, action)\n",
    "            \n",
    "            if is_terminal:\n",
    "                returnMemory = []\n",
    "                for hist_neutral_state, hist_action_probs, hist_player in memory:\n",
    "                    hist_outcome = value if hist_player == player else self.game.get_opponent_value(value)\n",
    "                    returnMemory.append((\n",
    "                        self.game.get_encoded_state(hist_neutral_state),\n",
    "                        hist_action_probs,\n",
    "                        hist_outcome\n",
    "                    ))\n",
    "                return returnMemory\n",
    "            \n",
    "            player = self.game.get_opponent(player)\n",
    "                \n",
    "    def train(self, memory):\n",
    "        random.shuffle(memory)\n",
    "        for batchIdx in range(0, len(memory), self.args['batch_size']):\n",
    "            sample = memory[batchIdx:min(len(memory) - 1, batchIdx + self.args['batch_size'])] # Change to memory[batchIdx:batchIdx+self.args['batch_size']] in case of an error\n",
    "            state, policy_targets, value_targets = zip(*sample)\n",
    "            \n",
    "            state, policy_targets, value_targets = np.array(state), np.array(policy_targets), np.array(value_targets).reshape(-1, 1)\n",
    "            \n",
    "            state = torch.tensor(state, dtype=torch.float32, device=self.model.device)\n",
    "            policy_targets = torch.tensor(policy_targets, dtype=torch.float32, device=self.model.device)\n",
    "            value_targets = torch.tensor(value_targets, dtype=torch.float32, device=self.model.device)\n",
    "            \n",
    "            out_policy, out_value = self.model(state)\n",
    "            \n",
    "            policy_loss = F.cross_entropy(out_policy, policy_targets)\n",
    "            value_loss = F.mse_loss(out_value, value_targets)\n",
    "            loss = policy_loss + value_loss\n",
    "            \n",
    "            optimizer.zero_grad() # change to self.optimizer\n",
    "            loss.backward()\n",
    "            optimizer.step() # change to self.optimizer\n",
    "    \n",
    "    def learn(self):\n",
    "        for iteration in range(self.args['num_iterations']):\n",
    "            memory = []\n",
    "            \n",
    "            self.model.eval()\n",
    "            for selfPlay_iteration in trange(self.args['num_selfPlay_iterations']):\n",
    "                memory += self.selfPlay()\n",
    "                \n",
    "            self.model.train()\n",
    "            for epoch in trange(self.args['num_epochs']):\n",
    "                self.train(memory)\n",
    "            \n",
    "            torch.save(self.model.state_dict(), f\"model_{iteration}_{self.game}.pt\")\n",
    "            torch.save(self.optimizer.state_dict(), f\"optimizer_{iteration}_{self.game}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bd91ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e28c8027603487ca195a97dd9dd2673",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "game = ConnectFour()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = ResNet(game, 9, 128, device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "\n",
    "args = {\n",
    "    'C': 2,\n",
    "    'num_searches': 600,\n",
    "    'num_iterations': 8,\n",
    "    'num_selfPlay_iterations': 1,\n",
    "    'num_parallel_games': 100,\n",
    "    'num_epochs': 4,\n",
    "    'batch_size': 128,\n",
    "    'temperature': 1.25,\n",
    "    'dirichlet_epsilon': 0.25,\n",
    "    'dirichlet_alpha': 0.3\n",
    "}\n",
    "\n",
    "alphaZero = AlphaZero(model, optimizer, game, args)\n",
    "alphaZero.learn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7c470145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]]\n",
      "valid_moves [0, 1, 2, 3, 4, 5, 6]\n",
      "[[0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1. -1.  0.  0.  0.  0.  0.]]\n",
      "valid_moves [0, 1, 2, 3, 4, 5, 6]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: ''",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 28\u001b[0m\n\u001b[1;32m     26\u001b[0m valid_moves \u001b[39m=\u001b[39m game\u001b[39m.\u001b[39mget_valid_moves(state)\n\u001b[1;32m     27\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mvalid_moves\u001b[39m\u001b[39m\"\u001b[39m, [i \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(game\u001b[39m.\u001b[39maction_size) \u001b[39mif\u001b[39;00m valid_moves[i] \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m])\n\u001b[0;32m---> 28\u001b[0m action \u001b[39m=\u001b[39m \u001b[39mint\u001b[39;49m(\u001b[39minput\u001b[39;49m(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m{\u001b[39;49;00mplayer\u001b[39m}\u001b[39;49;00m\u001b[39m:\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n\u001b[1;32m     30\u001b[0m \u001b[39mif\u001b[39;00m valid_moves[action] \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     31\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39maction not valid\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: ''"
     ]
    }
   ],
   "source": [
    "game = ConnectFour()\n",
    "player = 1\n",
    "\n",
    "args = {\n",
    "    'C': 2,\n",
    "    'num_searches': 100,\n",
    "    'dirichlet_epsilon': 0.,\n",
    "    'dirichlet_alpha': 0.3\n",
    "}\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = ResNet(game, 9, 128, device)\n",
    "# model.load_state_dict(torch.load(\"model_0_ConnectFour.pt\", map_location=device))\n",
    "model.eval()\n",
    "\n",
    "mcts = MCTS(game, args, model)\n",
    "\n",
    "state = game.get_initial_state()\n",
    "\n",
    "\n",
    "while True:\n",
    "    print(state)\n",
    "    \n",
    "    if player == 1:\n",
    "        valid_moves = game.get_valid_moves(state)\n",
    "        print(\"valid_moves\", [i for i in range(game.action_size) if valid_moves[i] == 1])\n",
    "        action = int(input(f\"{player}:\"))\n",
    "\n",
    "        if valid_moves[action] == 0:\n",
    "            print(\"action not valid\")\n",
    "            continue\n",
    "            \n",
    "    else:\n",
    "        neutral_state = game.change_perspective(state, player)\n",
    "        mcts_probs = mcts.search(neutral_state)\n",
    "        action = np.argmax(mcts_probs)\n",
    "        \n",
    "    state = game.get_next_state(state, action, player)\n",
    "    \n",
    "    value, is_terminal = game.get_value_and_terminated(state, action)\n",
    "    \n",
    "    if is_terminal:\n",
    "        print(state)\n",
    "        if value == 1:\n",
    "            print(player, \"won\")\n",
    "        else:\n",
    "            print(\"draw\")\n",
    "        break\n",
    "        \n",
    "    player = game.get_opponent(player)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
